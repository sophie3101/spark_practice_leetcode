from pyspark.sql import SparkSession
from pyspark.sql import functions as sf

spark = SparkSession.builder.appName("question1045").getOrCreate()
import pandas as pd

data = [[1, 5], [2, 6], [3, 5], [3, 6], [1, 6]]
customer = spark.createDataFrame(pd.DataFrame(data, columns=['customer_id', 'product_key']).astype({'customer_id':'Int64', 'product_key':'Int64'}))
data = [[5], [6]]
product = spark.createDataFrame(pd.DataFrame(data, columns=['product_key']).astype({'product_key':'Int64'}))

n_products=product.selectExpr("count (distinct product_key) as n_products").collect()[0]["n_products"]

customer.groupby('customer_id').\
    agg(sf.count('product_key').alias('num_products')).\
    filter(sf.col('num_products')==n_products).\
    select('customer_id').\
    show()